{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "아저씨가 삼촌이야? 그럼 아저씨는 삼촌표 간장을 만들어도 맛있겠다!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "print(llm.invoke('농담을 해봐.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 탁자 위에 고양이가 있다\n",
      "영어로 번역:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"탁자 위에 고양이가 있다\", language = \"영어\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1398368/1897775562.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/tmp/ipykernel_1398368/1897775562.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm_chain.run({\"sentence\": \"탁자 위에 고양이가 있다\", \"language\": \"영어\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a cat on the table\n"
     ]
    }
   ],
   "source": [
    "# LLMChain 초기화\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Inference 실행\n",
    "result = llm_chain.run({\"sentence\": \"탁자 위에 고양이가 있다\", \"language\": \"영어\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예제 CSV 파일 \"sample.csv\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['이름', '나이', '도시'],\n",
    "    ['존', 25, '뉴욕'],\n",
    "    ['에밀리', 28, '로스엔젤레스'],\n",
    "    ['미카엘', 22, '시카고']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# 데이터를 CSV 파일에 기록\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'예제 CSV 파일 \"{file_name}\"를 만들었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample.csv', 'row': 0}, page_content='이름: 존\\n나이: 25\\n도시: 뉴욕'), Document(metadata={'source': 'sample.csv', 'row': 1}, page_content='이름: 에밀리\\n나이: 28\\n도시: 로스엔젤레스'), Document(metadata={'source': 'sample.csv', 'row': 2}, page_content='이름: 미카엘\\n나이: 22\\n도시: 시카고')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 산과 자연에 대한 샘플 문장\n",
    "content = \"\"\"고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.\n",
    "청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\n",
    "자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.\n",
    "해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.\"\"\"\n",
    "\n",
    "# 파일명\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# 텍스트 파일에 내용 쓰기\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='고요한 풍경 속에서 우뚝 솟은 산들은 자연의 아름다움을 지키는 장엄한 수호자처럼 서 있습니다.'\n",
      "page_content='청량한 산 공기는 고요함의 속삭임을 전해주며, 바스락거리는 잎사귀들은 야생의 교향곡을 작곡합니다.\n",
      "자연의 팔레트는 산을 초록과 갈색의 색조로 칠해 경이로운 광경을 만들어냅니다.'\n",
      "page_content='해가 뜨면, 황금빛 광채가 산 정상에 비치며, 손길 닿지 않은 야생의 세계를 비춥니다.'\n"
     ]
    }
   ],
   "source": [
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베드된 문서:\n",
      "Number of vector: 5; Dimension of each vector: 1536\n",
      "임베드 질의:\n",
      "Dimension of the vector: 1536\n",
      "Sample of the first 5 elements of the vector: [-0.010684116743505001, -0.010173137299716473, -0.0019674645736813545, 0.023056013509631157, -0.02686513401567936]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model ='text-embedding-3-small')\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"임베드된 문서:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "print(\"임베드 질의:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 텍스트 파일 \"dialogue.txt\"를 만들었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 대화를 txt 파일에 저장\n",
    "# 대화 행 목록\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# 파일명\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# 대화 행들을 텍스트 파일에 기록\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'대화 텍스트 파일 \"{file_name}\"를 만들었습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 스토어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 문서를 로드해 청크들로 분할하고, 각 청크를 임베드해 벡터 스토어에 로드\n",
    "raw_documents = TextLoader('dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "# 고밀도 벡터의 효율적인 유사도 검색 및 클러스터링을 위해 메타 AI 리서치에서 개발한 FAISS 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to report an accident\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sorry to hear that. May I ask your name?' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 저장소에 검색기 '마운트'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reason of the call was to report an accident.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"What was the reason of the call?\"\n",
    "output = qa.invoke(query)\n",
    "output['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기억 (메모리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human is looking for ideas to write an essay about AI. The AI suggests writing about LLM.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
    "memory.save_context({\"input\": \"안녕하세요, AI에 관한 에세이를 쓸 아이디어를 찾고 있어요\"}, {\"output\": \"안녕하세요, LLM에 관해 써보면 어떨까요?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, Any]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/conda/envs/py310/lib/python3.10/site-packages/langchain/memory/summary.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체인\n",
    "\n",
    "체인(chain)은 일련의 작동과 LLM 호출을 미리 정해둔 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain\n",
    "\n",
    "가장 일반적인 유형의 체인.  \n",
    "프롬프트 템플릿, LLM, 출력 파서(옵션)로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' There is a cat on the table.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Chain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"탁자 위에 고양이가 있어요\", language=\"영어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a cat on the table.\n"
     ]
    }
   ],
   "source": [
    "# LLMChain deprecation 해결\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"문장: {sentence}\n",
    "{language}로 번역:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    {\n",
    "        \"sentence\": RunnablePassthrough(),\n",
    "        \"language\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"sentence\": \"탁자 위에 고양이가 있어요\",\n",
    "    \"language\": \"영어\"\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RouterChain\n",
    "\n",
    "특정 조건에 따라 입력 변수를 다른 체인으로 라우팅할 수 있는 체인 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1398368/347779221.py:45: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/pydantic/main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/tmp/ipykernel_1398368/347779221.py:57: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "itinerary_template = \"\"\"당신은 휴가 일정 조수입니다. \\\n",
    "당신은 고객이 최고의 목적지와 일정을 찾도록 도와줍니다. \\\n",
    "당신은 고객의 선호에 따라 최적화된 일정을 작성하는 데 도움을 줍니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"당신은 레스토랑 예약 조수입니다. \\\n",
    "당신은 고객의 손님 수와 음식 선호를 확인합니다. \\\n",
    "특별한 조건을 고려해야 하는지 주의합니다.\n",
    "\n",
    "여기에 질문이 있습니다:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"여행 일정\",\n",
    "        \"description\": \"여행 일정 작성을 돕습니다\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"레스토랑\",\n",
    "        \"description\": \"고객의 레스토랑 예약을 도와줍니다\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "여행 일정: {'input': '밀라노에서 베니스까지 자동차로 여행하려고 합니다.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '밀라노에서 베니스까지 자동차로 여행하려고 합니다.', 'text': ' 어떤 경로가 가장 좋을까요?\\n\\n일단 베니스는 이탈리아 북부에 위치하고 있으며, 이탈리아의 주요 도시 중 하나인 밀라노에서는 차로 약 3시간 거리에 있습니다. 따라서 베니스로 가는 가장 간단한 경로는 밀라노에서 A4 고속도로를 타고 동쪽으로 이동하는 것입니다. 이 도로는 밀라노에서 베니스까지 직접 이어져 있으며, 여러 번의 톨 게이트를 지나야 합니다.\\n\\n하지만 만약 당신이 좀 더 멋진 경험을 원한다면, 다른 루트를 고려해볼 수도 있습니다. 밀라노에서 베니스로 가는 도로 중 가장 유명하고 아름다운 도로는'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"밀라노에서 베니스까지 자동차로 여행하려고 합니다. 중간에 들릴 만한 명소가 있나요?.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "레스토랑: {'input': '고객의 레스토랑 예약을 도와줍니다오늘 저녁 식사를 예약하고 싶어요'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '고객의 레스토랑 예약을 도와줍니다오늘 저녁 식사를 예약하고 싶어요', 'text': '. 몇 명이 식사하실 건가요?\\n\\n고객님, 몇 분이 예약을 원하시는지 알려주시겠어요? 또한 음식 선호도 있나요? 예를 들어, 알러지나 식이 제한 사항이 있으신가요? 이러한 정보를 알려주시면 더 좋은 서비스를 제공해드릴 수 있어요. '}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"오늘 저녁 식사를 예약하고 싶어요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "\n",
    "여러 개의 체인을 순서대로 실행할 수 있는 체인 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=False prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='당신은 번역가입니다. 주어진 텍스트 입력을 {language}로 번역하세요.\\n번역:') llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f14e8b53670>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f14e89838b0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "# 주어진 주제에 대한 농담을 먼저 생성한 다음 -> 이를 다른 언어로 번역하는 AI 시스템\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 이 코드는 연극의 제목을 주면 시놉시스를 작성하는 LLMChain입니다.\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"당신은 코미디언입니다. {topic}에 관한 농담을 생성하세요.\n",
    "농담:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"당신은 번역가입니다. 주어진 텍스트 입력을 {language}로 번역하세요.\n",
    "번역:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "print(translator_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m \"고양이와 개가 싸우면 누가 이길까요? 당연히 고양이죠, 개는 꼬리를 쫓는 것밖에 못하니까요!\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m If cats and dogs fight, who do you think will win? Of course, it's the cat. Dogs can only chase their tails!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 이 코드는 두 체인을 순서대로 실행하는 전체 체인입니다.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"고양이와 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformationChain\n",
    "\n",
    "일부 함수나 표현식을 사용해 입력 변수나 다른 체인의 출력을 변환할 수 있는 체인 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string 모듈을 임포트합니다\n",
    "import string\n",
    "\n",
    "# 함수를 정의합니다\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "    # 파일을 읽기 모드로 엽니다\n",
    "    text = inputs[\"text\"]\n",
    "    # 'cat'을 'Silvester the Cat'으로 바꿉니다\n",
    "    new_text = text.replace('cat', 'Silvester the Cat')\n",
    "    # 변경된 텍스트를 반환합니다\n",
    "    return {\"output_text\": new_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"\\nThe Cat and the Dog\\n\\nThere was once a cat and a dog who lived in the same house. They did not get along very well, as they often fought over food, toys, and attention. The cat was clever and cunning, while the dog was loyal and friendly. The cat liked to tease the dog, and the dog liked to chase the cat.\\n\\nOne day, the cat decided to play a prank on the dog. He found a ball of yarn and tied it around the dog's tail. Then he hid behind a sofa and waited for the dog to notice. When the dog saw the yarn, he thought it was a toy and started to play with it. He ran around the house, trying to catch the yarn with his mouth. But every time he got close, the yarn moved away from him. The cat laughed silently as he watched the dog's futile attempts.\\n\\nThe dog soon realized that something was wrong. He looked behind him and saw that the yarn was attached to his tail. He tried to pull it off, but it was too tight. He felt angry and embarrassed. He wondered who did this to him. He sniffed the air and detected the cat's scent. He knew it was the cat who tricked him. He growled and ran towards the sofa where the cat was hiding.\\n\\nThe cat heard the dog's growl and saw him coming. He panicked and ran away from the sofa. He hoped to find a safe place to hide, but he was too late. The dog was faster and caught up with him. He grabbed the cat by the scruff of his neck and shook him hard. The cat yowled and scratched the dog's face. The dog barked and bit the cat's ear. They rolled on the floor, biting and clawing each other.\\n\\nThe noise they made woke up their owner, who was sleeping upstairs. She came down and saw them fighting. She was shocked and angry. She shouted at them to stop. She grabbed a broom and hit them lightly on their heads. They stopped fighting and looked at her with fear. She scolded them for being naughty and making a mess. She untied the yarn from the dog's tail and threw it away. She took them to the bathroom and cleaned their wounds. She told them to behave themselves and get along.\\n\\nThe cat and the dog felt ashamed of themselves. They realized that they had hurt each other and their owner. They apologized to each other and to their owner. They promised to be nicer to each other and share their things. They hugged each other and licked each other's faces.\\n\\nFrom that day on, they became friends. They played together, slept together, and ate together. They learned to respect each other's differences and appreciate each other's strengths. They were happy and content.\\n\\nThe end.\",\n",
       " 'output': ' 실베스터 고양이와 개가 같은 집에서 살았으나 음식, 장난감, 주인의 관심 등을 놓고 자주 싸웠다. 고양이는 남을 놀리는 것을 좋아하고, 개는 충성스럽고 친절했다. 어느 날 고양이가 개를 속이려다 실패하고 싸우게 되었고, 주인에게 혼나게 되었다. 하지만 이를 계기로 서로를 이해하고 친구가 되었다.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "import string\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"이 텍스트를 요약하세요:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "요약:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.invoke(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent (에이전트)\n",
    "\n",
    "에이전트는 LLM 기반 애플리케이션 내에서 의사 결정을 내리는 주체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1398368/1685785234.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m 아바타 2 개봉일을 알고 싶다면, 검색 도구를 사용하는 것이 좋다.\n",
      "Action: Search\n",
      "Action Input: \"아바타 2 개봉일\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDecember 16, 2022\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 검색 결과로 2022년 12월 16일이 나왔다.\n",
      "Final Answer: December 16, 2022\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '아바타 2 개봉일은?', 'output': 'December 16, 2022'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"현재 이벤트에 관해 질문할 때 유용함\"\n",
    "    )]\n",
    "\n",
    "agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.invoke(\"아바타 2 개봉일은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.60-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langgraph) (0.3.29)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.13)\n",
      "Requirement already satisfied: anyio in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/haiqv/conda/envs/py310/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.2.60-py3-none-any.whl (135 kB)\n",
      "Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
      "Downloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
      "Successfully installed langgraph-0.2.60 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.48\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "아바타 2 개봉일은?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Search (call_5rMEeD6YOjFSk6mw5fzFO1qF)\n",
      " Call ID: call_5rMEeD6YOjFSk6mw5fzFO1qF\n",
      "  Args:\n",
      "    __arg1: 아바타 2 개봉일\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: Search\n",
      "\n",
      "December 16, 2022\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "영화 \"아바타 2\"는 2022년 12월 16일에 개봉되었습니다.\n"
     ]
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"현재 이벤트에 관해 질문할 때 유용함\"\n",
    "    )]\n",
    "\n",
    "# agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.invoke(\"아바타 2 개봉일은?\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "graph = create_react_agent(model, tools=tools)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"아바타 2 개봉일은?\")]}\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
